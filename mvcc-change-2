# O_DIRECT-Friendly Block Allocator (256 KiB Version)

Purpose: allocate offsets and lengths so every write/read is `O_DIRECT`-safe, minimize internal fragmentation for variable-sized payloads, and roll over to a new file when one gets too big. Make the block size a configurable constant.

---

## 1. Config

Define these at the top:

```text
const ALIGN_4K        = 4096;
const BLOCK_SIZE      = 256 * 1024;   // 256 KiB, can be changed
const FILE_MAX_BYTES  = 4ull * 1024 * 1024 * 1024; // 4 GiB, can be changed
```

Requirements:

* `BLOCK_SIZE % ALIGN_4K == 0` must hold (256 KiB % 4 KiB = 0).
* If someone changes `BLOCK_SIZE`, validate it once at startup.

---

## 2. Core idea

For a payload of arbitrary size `actual_len`:

1. Carve out as many full `BLOCK_SIZE` chunks as possible.
2. For the remainder, round **up** to 4 KiB.
3. Allocate that many bytes from the current file.
4. Always keep file offsets 4 KiB-aligned.
5. Store both `actual_len` and `alloc_len` in the index.

So a segment in a file looks like:

```text
[full 256 KiB blocks][4 KiB–padded tail]
```

Next segment starts immediately after that → still 4 KiB aligned.

---

## 3. Allocation math

Given `actual_len`:

```text
full_blocks = actual_len / BLOCK_SIZE
tail        = actual_len % BLOCK_SIZE

alloc_len = full_blocks * BLOCK_SIZE
if tail > 0:
    alloc_len += round_up_4k(tail)
```

Rounding:

```text
round_up_4k(x) = (x + (ALIGN_4K - 1)) & ~(ALIGN_4K - 1)
```

Properties:

* If `tail == 0` → `alloc_len` is exactly multiple of 256 KiB → OK for `O_DIRECT`.
* If `tail > 0` → extra waste ≤ 4 KiB.

Example:

* `actual_len = 900,000 bytes`

  * `BLOCK_SIZE = 262,144`
  * `full_blocks = 900,000 / 262,144 = 3` (int)
  * bytes covered = 3 * 262,144 = 786,432
  * `tail = 900,000 - 786,432 = 113,568`
  * `round_up_4k(113,568)` → 113,568 % 4,096 = 2,848 → pad = 1,248 → tail_alloc = 114,816
  * `alloc_len = 786,432 + 114,816 = 901,248`
  * waste = 1,248 bytes

So even with weird sizes, waste is sub-4 KiB.

---

## 4. File state

```text
struct FileState {
    u32 file_id;
    u64 current_offset;   // always 4 KiB aligned
    u64 max_size_bytes;   // e.g. FILE_MAX_BYTES
    int fd;               // O_DIRECT | O_RDWR
};
```

Invariant:

```text
current_offset % ALIGN_4K == 0
```

Init:

```text
file_id = 0;
current_offset = 0;
max_size_bytes = FILE_MAX_BYTES;
```

---

## 5. Allocation algorithm (with rollover)

```text
AllocationResult alloc(u64 actual_len) {
    u64 full_blocks = actual_len / BLOCK_SIZE;
    u64 tail        = actual_len % BLOCK_SIZE;

    u64 alloc_len = full_blocks * BLOCK_SIZE;
    if (tail > 0) {
        alloc_len += round_up_4k(tail);
    }

    // need to ensure we fit in current file
    if (current_offset + alloc_len > max_size_bytes) {
        rotate_to_new_file();                 // file_id++, current_offset=0, open+fallocate
    }

    u64 offset = current_offset;
    current_offset += alloc_len;

    return (AllocationResult){
        .file_id    = file_id,
        .offset     = offset,
        .actual_len = actual_len,
        .alloc_len  = alloc_len
    };
}
```

`rotate_to_new_file()`:

1. close current fd
2. open new path like `data.<file_id+1>`
3. open with `O_DIRECT | O_RDWR | O_CREAT`
4. optional: `fallocate(fd, 0, 0, FILE_MAX_BYTES)`
5. set `current_offset = 0`, increment `file_id`

All under a mutex so multiple threads don’t rotate twice.

---

## 6. Write path (io_uring)

Given the allocation result `ar` and user buffer `buf` (`len = ar.actual_len`):

```text
write_payload(ar, buf):
    fd  = open_files[ar.file_id]
    off = ar.offset
    rem = ar.actual_len

    // write full 256 KiB blocks
    while rem >= BLOCK_SIZE:
        sqe = prep_write(fd, buf, BLOCK_SIZE, off)
        submit_later(sqe)        // don't flush yet
        buf += BLOCK_SIZE
        off += BLOCK_SIZE
        rem -= BLOCK_SIZE

    // tail
    if rem > 0:
        tail_alloc = round_up_4k(rem)
        tmp = get_4k_aligned_buffer(tail_alloc)
        memcpy(tmp, buf, rem)
        memset(tmp + rem, 0, tail_alloc - rem)
        sqe = prep_write(fd, tmp, tail_alloc, off)
        submit_later(sqe)

    // elsewhere: flush in batches
```

Notes:

* We’re not relying on “kernel auto-pulling”; we explicitly call `io_uring_submit()` after accumulating SQEs.
* Because BLOCK_SIZE is 256 KiB, a large payload (say 8 MiB) becomes 32 SQEs of 256 KiB. That’s fine because we batch.
* If you want fewer SQEs, you can coalesce 4 × 256 KiB in userspace and write 1 MiB — but the allocator itself doesn’t need that.

---

## 7. Read path

From index we have:

* `file_id`
* `offset` (4 KiB aligned)
* `actual_len`
* `alloc_len` (4 KiB aligned)

Read:

1. allocate 4 KiB–aligned buffer of `alloc_len`
2. submit 1 read with `O_DIRECT`: `(fd, offset, alloc_len)`
3. on completion, return only first `actual_len` bytes

This symmetry is why we store `alloc_len`.

---

## 8. Index entry

```text
struct IndexEntry {
    u32 file_id;
    u64 offset;       // 4 KiB aligned
    u32 actual_len;   // real payload size
    u32 alloc_len;    // what we actually wrote, multiple of 4 KiB
};
```

Optional fields: checksum, compression, version.

Storing `alloc_len` makes future block-size changes safer: old entries still readable.

---

## 9. Concurrency

Allocation is the only part that must be serialized per file.

Simplest:

```text
mutex lock
ar = current_file.alloc(actual_len)
unlock
// now do IO without holding lock
```

Because `alloc_len` is already aligned, nothing else needs to coordinate on offsets.

If you want to make it mostly lock-free:

* do allocation under lock
* I/O (io_uring submit) is per-thread
* have 1 reaper thread for CQEs

---

## 10. File creation / preallocation

On `rotate_to_new_file()`:

```text
path = format("data.%u", file_id + 1)
fd = open(path, O_CREAT | O_DIRECT | O_RDWR, 0644)
fallocate(fd, 0, 0, FILE_MAX_BYTES)   // optional but recommended
open_files[file_id + 1] = fd
```

This keeps extents contiguous and performance predictable.

---

## 11. Invariants to assert (debug)

```text
assert(offset % ALIGN_4K == 0);
assert(alloc_len % ALIGN_4K == 0);
assert(BLOCK_SIZE % ALIGN_4K == 0);
```

If a write fails with `-EINVAL`, print:

* fd
* offset
* len
* pointer addr

---

## 12. Why 256 KiB

* Still large enough to amortize `O_DIRECT` overhead on NVMe.
* Better space efficiency for variable-sized payloads than 1 MiB.
* Easy to change: set `BLOCK_SIZE` to 512 KiB or 1 MiB and the rest of the logic stays the same.
* Because everything is derived off `BLOCK_SIZE` and `ALIGN_4K`, changing the constant is just a rebuild.

---

## 13. Summary

* Alignment cost: ≤ 4 KiB per payload.
* I/O size: multiples of 256 KiB + small padded tail.
* Offsets: always 4 KiB aligned.
* Rollover: deterministic at `FILE_MAX_BYTES`.
* Configurable: change `BLOCK_SIZE` const and everything else recomputes.
