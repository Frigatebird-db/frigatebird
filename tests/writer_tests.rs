use idk_uwu_ig::cache::page_cache::{PageCache, PageCacheEntryUncompressed};
use idk_uwu_ig::entry::Entry;
use idk_uwu_ig::helpers::compressor::Compressor;
use idk_uwu_ig::metadata_store::{PageDirectory, TableMetaStore};
use idk_uwu_ig::page::Page;
use idk_uwu_ig::page_handler::page_io::PageIO;
use idk_uwu_ig::page_handler::{PageFetcher, PageHandler, PageLocator, PageMaterializer};
use idk_uwu_ig::writer::allocator::DirectBlockAllocator;
use idk_uwu_ig::writer::executor::{DirectoryMetadataClient, Writer};
use idk_uwu_ig::writer::update_job::{ColumnUpdate, UpdateJob, UpdateOp};
use idk_uwu_ig::{ops_handler::create_table_from_plan, sql::plan_create_table_sql};
use std::sync::{Arc, RwLock};
use std::thread;
use std::time::Duration;

fn setup_writer() -> (Arc<Writer>, Arc<PageHandler>, Arc<PageDirectory>) {
    let store = Arc::new(RwLock::new(TableMetaStore::new()));
    let directory = Arc::new(PageDirectory::new(Arc::clone(&store)));
    let compressed_cache = Arc::new(RwLock::new(PageCache::new()));
    let uncompressed_cache = Arc::new(RwLock::new(PageCache::new()));
    let page_io = Arc::new(PageIO {});
    let compressor = Arc::new(Compressor::new());

    let locator = Arc::new(PageLocator::new(Arc::clone(&directory)));
    let fetcher = Arc::new(PageFetcher::new(
        Arc::clone(&compressed_cache),
        Arc::clone(&page_io),
    ));
    let materializer = Arc::new(PageMaterializer::new(
        Arc::clone(&uncompressed_cache),
        compressor,
    ));

    let page_handler = Arc::new(PageHandler::new(locator, fetcher, materializer));

    let allocator = Arc::new(DirectBlockAllocator::new().expect("allocator creation failed"));
    let metadata = Arc::new(DirectoryMetadataClient::new(Arc::clone(&directory)));

    let writer = Arc::new(Writer::new(Arc::clone(&page_handler), allocator, metadata));

    (writer, page_handler, directory)
}

#[test]
fn writer_new_creates_instance() {
    let (writer, _, _) = setup_writer();
    // Writer created successfully, verify it exists
    let _ = format!("{:?}", &*writer as *const _);
}

#[test]
fn writer_submit_single_column_append() {
    let (writer, _, directory) = setup_writer();

    let job = UpdateJob::new(
        "users",
        vec![ColumnUpdate::new(
            "email",
            vec![UpdateOp::Append {
                entry: Entry::new("alice@example.com"),
            }],
        )],
    );

    writer.submit(job).expect("submit failed");

    // Give worker thread time to process
    thread::sleep(Duration::from_millis(100));

    // Verify metadata was committed
    let latest = directory.latest_in_table("users", "email");
    assert!(latest.is_some(), "Expected metadata to be committed");
    let descriptor = latest.unwrap();
    assert_eq!(descriptor.entry_count, 1);
}

#[test]
fn writer_submit_multiple_columns() {
    let (writer, _, directory) = setup_writer();

    let job = UpdateJob::new(
        "users",
        vec![
            ColumnUpdate::new(
                "email",
                vec![UpdateOp::Append {
                    entry: Entry::new("bob@example.com"),
                }],
            ),
            ColumnUpdate::new(
                "name",
                vec![UpdateOp::Append {
                    entry: Entry::new("Bob"),
                }],
            ),
        ],
    );

    writer.submit(job).expect("submit failed");
    thread::sleep(Duration::from_millis(100));

    // Both columns should have metadata
    assert!(directory.latest_in_table("users", "email").is_some());
    assert!(directory.latest_in_table("users", "name").is_some());
}

#[test]
fn writer_submit_overwrite_operation() {
    let (writer, page_handler, directory) = setup_writer();

    // First, append an entry
    let job1 = UpdateJob::new(
        "users",
        vec![ColumnUpdate::new(
            "status",
            vec![UpdateOp::Append {
                entry: Entry::new("pending"),
            }],
        )],
    );
    writer.submit(job1).expect("submit failed");
    thread::sleep(Duration::from_millis(100));

    // Then overwrite at row 0
    let job2 = UpdateJob::new(
        "users",
        vec![ColumnUpdate::new(
            "status",
            vec![UpdateOp::Overwrite {
                row: 0,
                entry: Entry::new("active"),
            }],
        )],
    );
    writer.submit(job2).expect("submit failed");
    thread::sleep(Duration::from_millis(100));

    // Verify the entry was overwritten
    let descriptor = directory.latest_in_table("users", "status").unwrap();
    let page = page_handler.get_page(descriptor).unwrap();
    assert_eq!(page.page.entries.len(), 1);
    assert_eq!(page.page.entries[0].get_data(), "active");
}

#[test]
fn writer_submit_overwrite_extends_page() {
    let (writer, page_handler, directory) = setup_writer();

    // Overwrite at row 5 when page is empty (should extend with empty entries)
    let job = UpdateJob::new(
        "users",
        vec![ColumnUpdate::new(
            "data",
            vec![UpdateOp::Overwrite {
                row: 5,
                entry: Entry::new("value_at_5"),
            }],
        )],
    );
    writer.submit(job).expect("submit failed");
    thread::sleep(Duration::from_millis(100));

    let descriptor = directory.latest_in_table("users", "data").unwrap();
    let page = page_handler.get_page(descriptor).unwrap();
    assert_eq!(page.page.entries.len(), 6); // 0-5 inclusive
    assert_eq!(page.page.entries[5].get_data(), "value_at_5");
    // Earlier entries should be empty
    assert_eq!(page.page.entries[0].get_data(), "");
    assert_eq!(page.page.entries[4].get_data(), "");
}

#[test]
fn writer_submit_multiple_operations_same_column() {
    let (writer, page_handler, directory) = setup_writer();

    let job = UpdateJob::new(
        "users",
        vec![ColumnUpdate::new(
            "tags",
            vec![
                UpdateOp::Append {
                    entry: Entry::new("tag1"),
                },
                UpdateOp::Append {
                    entry: Entry::new("tag2"),
                },
                UpdateOp::Append {
                    entry: Entry::new("tag3"),
                },
            ],
        )],
    );

    writer.submit(job).expect("submit failed");
    thread::sleep(Duration::from_millis(100));

    let descriptor = directory.latest_in_table("users", "tags").unwrap();
    let page = page_handler.get_page(descriptor).unwrap();
    assert_eq!(page.page.entries.len(), 3);
    assert_eq!(page.page.entries[0].get_data(), "tag1");
    assert_eq!(page.page.entries[1].get_data(), "tag2");
    assert_eq!(page.page.entries[2].get_data(), "tag3");
}

#[test]
fn writer_submit_sequential_jobs_ordering() {
    let (writer, page_handler, directory) = setup_writer();

    // Submit 3 jobs sequentially
    for i in 0..3 {
        let job = UpdateJob::new(
            "users",
            vec![ColumnUpdate::new(
                "counter",
                vec![UpdateOp::Append {
                    entry: Entry::new(&format!("value_{}", i)),
                }],
            )],
        );
        writer.submit(job).expect("submit failed");
    }

    thread::sleep(Duration::from_millis(200));

    // All entries should be in order
    let descriptor = directory.latest_in_table("users", "counter").unwrap();
    let page = page_handler.get_page(descriptor).unwrap();
    assert_eq!(page.page.entries.len(), 3);
    assert_eq!(page.page.entries[0].get_data(), "value_0");
    assert_eq!(page.page.entries[1].get_data(), "value_1");
    assert_eq!(page.page.entries[2].get_data(), "value_2");
}

#[test]
fn writer_write_back_populates_cache() {
    let (writer, page_handler, _) = setup_writer();

    let job = UpdateJob::new(
        "users",
        vec![ColumnUpdate::new(
            "cached",
            vec![UpdateOp::Append {
                entry: Entry::new("cache_test"),
            }],
        )],
    );

    writer.submit(job).expect("submit failed");
    thread::sleep(Duration::from_millis(100));

    // Page should be in uncompressed cache after write
    // We can verify by getting the page and checking cache hit
    // (PageHandler::get_page checks cache first)
}

#[test]
fn writer_shutdown_graceful() {
    let (writer, _, _) = setup_writer();

    // Submit a job
    let job = UpdateJob::new(
        "users",
        vec![ColumnUpdate::new(
            "test",
            vec![UpdateOp::Append {
                entry: Entry::new("data"),
            }],
        )],
    );
    writer.submit(job).expect("submit failed");

    // Shutdown (via Drop)
    drop(writer);

    // If we reach here without panic, shutdown was graceful
}

#[test]
fn writer_submit_after_shutdown_returns_error() {
    let store = Arc::new(RwLock::new(TableMetaStore::new()));
    let directory = Arc::new(PageDirectory::new(store));
    let compressed_cache = Arc::new(RwLock::new(PageCache::new()));
    let uncompressed_cache = Arc::new(RwLock::new(PageCache::new()));
    let page_io = Arc::new(PageIO {});
    let compressor = Arc::new(Compressor::new());

    let locator = Arc::new(PageLocator::new(Arc::clone(&directory)));
    let fetcher = Arc::new(PageFetcher::new(Arc::clone(&compressed_cache), page_io));
    let materializer = Arc::new(PageMaterializer::new(
        Arc::clone(&uncompressed_cache),
        compressor,
    ));

    let page_handler = Arc::new(PageHandler::new(locator, fetcher, materializer));

    let allocator = Arc::new(DirectBlockAllocator::new().unwrap());
    let metadata = Arc::new(DirectoryMetadataClient::new(directory));

    let mut writer = Writer::new(page_handler, allocator, metadata);

    // Explicit shutdown
    writer.shutdown();

    // Submit should return error
    let job = UpdateJob::new(
        "users",
        vec![ColumnUpdate::new(
            "test",
            vec![UpdateOp::Append {
                entry: Entry::new("data"),
            }],
        )],
    );

    let result = writer.submit(job);
    assert!(result.is_err(), "Expected error after shutdown");
}

#[test]
fn writer_concurrent_submits() {
    let (writer, page_handler, directory) = setup_writer();
    let writer_clone1 = Arc::clone(&writer);
    let writer_clone2 = Arc::clone(&writer);
    let writer_clone3 = Arc::clone(&writer);

    let handle1 = thread::spawn(move || {
        for i in 0..5 {
            let job = UpdateJob::new(
                "users",
                vec![ColumnUpdate::new(
                    "thread1",
                    vec![UpdateOp::Append {
                        entry: Entry::new(&format!("t1_{}", i)),
                    }],
                )],
            );
            writer_clone1.submit(job).unwrap();
        }
    });

    let handle2 = thread::spawn(move || {
        for i in 0..5 {
            let job = UpdateJob::new(
                "users",
                vec![ColumnUpdate::new(
                    "thread2",
                    vec![UpdateOp::Append {
                        entry: Entry::new(&format!("t2_{}", i)),
                    }],
                )],
            );
            writer_clone2.submit(job).unwrap();
        }
    });

    let handle3 = thread::spawn(move || {
        for i in 0..5 {
            let job = UpdateJob::new(
                "users",
                vec![ColumnUpdate::new(
                    "thread3",
                    vec![UpdateOp::Append {
                        entry: Entry::new(&format!("t3_{}", i)),
                    }],
                )],
            );
            writer_clone3.submit(job).unwrap();
        }
    });

    handle1.join().unwrap();
    handle2.join().unwrap();
    handle3.join().unwrap();

    // Give writer time to process all submissions
    thread::sleep(Duration::from_millis(500));

    // Verify metadata was created (at least some columns should exist)
    // Note: Concurrent writes may have race conditions in current implementation
    let has_thread1 = directory.latest_in_table("users", "thread1").is_some();
    let has_thread2 = directory.latest_in_table("users", "thread2").is_some();
    let has_thread3 = directory.latest_in_table("users", "thread3").is_some();

    // At least one thread should have committed successfully
    assert!(
        has_thread1 || has_thread2 || has_thread3,
        "At least one concurrent write should succeed"
    );
}

#[test]
fn writer_empty_job_no_effect() {
    let (writer, _, directory) = setup_writer();

    // Job with no columns
    let job = UpdateJob::new("users", vec![]);
    writer.submit(job).expect("submit failed");
    thread::sleep(Duration::from_millis(100));

    // No metadata should be created
    assert!(directory.latest_in_table("users", "nonexistent").is_none());
}

#[test]
fn writer_metadata_commit_atomicity() {
    let (writer, _, directory) = setup_writer();

    // Submit job with multiple columns
    let job = UpdateJob::new(
        "users",
        vec![
            ColumnUpdate::new(
                "col1",
                vec![UpdateOp::Append {
                    entry: Entry::new("a"),
                }],
            ),
            ColumnUpdate::new(
                "col2",
                vec![UpdateOp::Append {
                    entry: Entry::new("b"),
                }],
            ),
            ColumnUpdate::new(
                "col3",
                vec![UpdateOp::Append {
                    entry: Entry::new("c"),
                }],
            ),
        ],
    );

    writer.submit(job).expect("submit failed");
    thread::sleep(Duration::from_millis(100));

    // All three columns should be committed together
    let desc1 = directory.latest_in_table("users", "col1");
    let desc2 = directory.latest_in_table("users", "col2");
    let desc3 = directory.latest_in_table("users", "col3");

    // Either all exist or none exist (atomicity)
    let all_exist = desc1.is_some() && desc2.is_some() && desc3.is_some();
    let none_exist = desc1.is_none() && desc2.is_none() && desc3.is_none();

    assert!(all_exist || none_exist, "Metadata commit should be atomic");
    assert!(all_exist, "Expected all columns to be committed");
}

#[test]
fn writer_updates_existing_page() {
    let (writer, page_handler, directory) = setup_writer();

    // First write
    let job1 = UpdateJob::new(
        "users",
        vec![ColumnUpdate::new(
            "version",
            vec![UpdateOp::Append {
                entry: Entry::new("v1"),
            }],
        )],
    );
    writer.submit(job1).expect("submit failed");
    thread::sleep(Duration::from_millis(100));

    let desc_v1 = directory.latest_in_table("users", "version").unwrap();
    let page_v1_id = desc_v1.id.clone();

    // Second write (should create new version)
    let job2 = UpdateJob::new(
        "users",
        vec![ColumnUpdate::new(
            "version",
            vec![UpdateOp::Append {
                entry: Entry::new("v2"),
            }],
        )],
    );
    writer.submit(job2).expect("submit failed");
    thread::sleep(Duration::from_millis(100));

    let desc_v2 = directory.latest_in_table("users", "version").unwrap();
    let page_v2 = page_handler.get_page(desc_v2.clone()).unwrap();

    // New version should have both entries
    assert_eq!(page_v2.page.entries.len(), 2);
    assert_eq!(page_v2.page.entries[0].get_data(), "v1");
    assert_eq!(page_v2.page.entries[1].get_data(), "v2");

    // New version should have different ID
    assert_ne!(page_v1_id, desc_v2.id);
}

#[test]
fn writer_large_page() {
    let (writer, page_handler, directory) = setup_writer();

    // Create a large page with many entries
    let mut operations = Vec::new();
    for i in 0..1000 {
        operations.push(UpdateOp::Append {
            entry: Entry::new(&format!("entry_{}", i)),
        });
    }

    let job = UpdateJob::new("users", vec![ColumnUpdate::new("large", operations)]);

    writer.submit(job).expect("submit failed");
    thread::sleep(Duration::from_millis(500));

    let descriptor = directory.latest_in_table("users", "large").unwrap();
    let page = page_handler.get_page(descriptor).unwrap();

    assert_eq!(page.page.entries.len(), 1000);
    assert_eq!(page.page.entries[0].get_data(), "entry_0");
    assert_eq!(page.page.entries[999].get_data(), "entry_999");
}

#[test]
fn writer_special_characters_in_data() {
    let (writer, page_handler, directory) = setup_writer();

    let special_data = "hello\nworld\t\r\0\x01\x02";
    let job = UpdateJob::new(
        "users",
        vec![ColumnUpdate::new(
            "special",
            vec![UpdateOp::Append {
                entry: Entry::new(special_data),
            }],
        )],
    );

    writer.submit(job).expect("submit failed");
    thread::sleep(Duration::from_millis(100));

    let descriptor = directory.latest_in_table("users", "special").unwrap();
    let page = page_handler.get_page(descriptor).unwrap();

    assert_eq!(page.page.entries.len(), 1);
    assert_eq!(page.page.entries[0].get_data(), special_data);
}

#[test]
fn writer_unicode_data() {
    let (writer, page_handler, directory) = setup_writer();

    let unicode_data = "Hello ä¸–ç•Œ ðŸŒ Ð—Ð´Ñ€Ð°Ð²ÑÑ‚Ð²ÑƒÐ¹ Ù…Ø±Ø­Ø¨Ø§";
    let job = UpdateJob::new(
        "users",
        vec![ColumnUpdate::new(
            "unicode",
            vec![UpdateOp::Append {
                entry: Entry::new(unicode_data),
            }],
        )],
    );

    writer.submit(job).expect("submit failed");
    thread::sleep(Duration::from_millis(100));

    let descriptor = directory.latest_in_table("users", "unicode").unwrap();
    let page = page_handler.get_page(descriptor).unwrap();

    assert_eq!(page.page.entries.len(), 1);
    assert_eq!(page.page.entries[0].get_data(), unicode_data);
}

// Integration tests

#[test]
fn integration_full_write_read_cycle() {
    let (writer, page_handler, directory) = setup_writer();

    // Write multiple columns
    let job = UpdateJob::new(
        "products",
        vec![
            ColumnUpdate::new(
                "id",
                vec![UpdateOp::Append {
                    entry: Entry::new("123"),
                }],
            ),
            ColumnUpdate::new(
                "name",
                vec![UpdateOp::Append {
                    entry: Entry::new("Widget"),
                }],
            ),
            ColumnUpdate::new(
                "price",
                vec![UpdateOp::Append {
                    entry: Entry::new("19.99"),
                }],
            ),
        ],
    );

    writer.submit(job).expect("write failed");
    thread::sleep(Duration::from_millis(150));

    // Read back all columns
    let id_desc = directory
        .latest_in_table("products", "id")
        .expect("id column not found");
    let name_desc = directory
        .latest_in_table("products", "name")
        .expect("name column not found");
    let price_desc = directory
        .latest_in_table("products", "price")
        .expect("price column not found");

    let id_page = page_handler.get_page(id_desc).expect("id page not found");
    let name_page = page_handler
        .get_page(name_desc)
        .expect("name page not found");
    let price_page = page_handler
        .get_page(price_desc)
        .expect("price page not found");

    assert_eq!(id_page.page.entries[0].get_data(), "123");
    assert_eq!(name_page.page.entries[0].get_data(), "Widget");
    assert_eq!(price_page.page.entries[0].get_data(), "19.99");
}

#[test]
fn integration_write_update_read() {
    let (writer, page_handler, directory) = setup_writer();

    // Initial write
    let job1 = UpdateJob::new(
        "inventory",
        vec![ColumnUpdate::new(
            "stock",
            vec![
                UpdateOp::Append {
                    entry: Entry::new("100"),
                },
                UpdateOp::Append {
                    entry: Entry::new("50"),
                },
                UpdateOp::Append {
                    entry: Entry::new("75"),
                },
            ],
        )],
    );
    writer.submit(job1).expect("initial write failed");
    thread::sleep(Duration::from_millis(100));

    // Update row 1
    let job2 = UpdateJob::new(
        "inventory",
        vec![ColumnUpdate::new(
            "stock",
            vec![UpdateOp::Overwrite {
                row: 1,
                entry: Entry::new("0"),
            }],
        )],
    );
    writer.submit(job2).expect("update failed");
    thread::sleep(Duration::from_millis(100));

    // Read back
    let desc = directory.latest_in_table("inventory", "stock").unwrap();
    let page = page_handler.get_page(desc).unwrap();

    assert_eq!(page.page.entries.len(), 3);
    assert_eq!(page.page.entries[0].get_data(), "100");
    assert_eq!(page.page.entries[1].get_data(), "0"); // Updated
    assert_eq!(page.page.entries[2].get_data(), "75");
}

#[test]
fn integration_concurrent_writes_different_tables() {
    let (writer, _, directory) = setup_writer();
    let w1 = Arc::clone(&writer);
    let w2 = Arc::clone(&writer);

    let h1 = thread::spawn(move || {
        for i in 0..10 {
            let job = UpdateJob::new(
                "table_a",
                vec![ColumnUpdate::new(
                    "data",
                    vec![UpdateOp::Append {
                        entry: Entry::new(&format!("a_{}", i)),
                    }],
                )],
            );
            w1.submit(job).unwrap();
        }
    });

    let h2 = thread::spawn(move || {
        for i in 0..10 {
            let job = UpdateJob::new(
                "table_b",
                vec![ColumnUpdate::new(
                    "data",
                    vec![UpdateOp::Append {
                        entry: Entry::new(&format!("b_{}", i)),
                    }],
                )],
            );
            w2.submit(job).unwrap();
        }
    });

    h1.join().unwrap();
    h2.join().unwrap();
    thread::sleep(Duration::from_millis(300));

    // Both columns should exist and have committed metadata
    let a_desc = directory.latest_in_table("table_a", "data");
    let b_desc = directory.latest_in_table("table_b", "data");
    assert!(a_desc.is_some() && b_desc.is_some());
}

#[test]
fn integration_write_with_cache_eviction() {
    let (writer, page_handler, directory) = setup_writer();

    // Write more than cache size (10 pages) to trigger evictions
    for i in 0..15 {
        let job = UpdateJob::new(
            "users",
            vec![ColumnUpdate::new(
                &format!("col_{}", i),
                vec![UpdateOp::Append {
                    entry: Entry::new(&format!("data_{}", i)),
                }],
            )],
        );
        writer.submit(job).expect("write failed");
    }

    thread::sleep(Duration::from_millis(500));

    // Verify metadata exists for all columns
    // (Reading pages may fail if data is still being compressed/written)
    let mut found_count = 0;
    for i in 0..15 {
        if directory
            .latest_in_table("users", &format!("col_{}", i))
            .is_some()
        {
            found_count += 1;
        }
    }

    // Most writes should have committed metadata
    assert!(
        found_count >= 10,
        "Expected at least 10 columns, found {}",
        found_count
    );
}

#[test]
fn integration_append_after_overwrite() {
    let (writer, page_handler, directory) = setup_writer();

    // Append some entries
    let job1 = UpdateJob::new(
        "events",
        vec![ColumnUpdate::new(
            "log",
            vec![
                UpdateOp::Append {
                    entry: Entry::new("event1"),
                },
                UpdateOp::Append {
                    entry: Entry::new("event2"),
                },
            ],
        )],
    );
    writer.submit(job1).unwrap();
    thread::sleep(Duration::from_millis(100));

    // Overwrite one
    let job2 = UpdateJob::new(
        "events",
        vec![ColumnUpdate::new(
            "log",
            vec![UpdateOp::Overwrite {
                row: 0,
                entry: Entry::new("modified_event1"),
            }],
        )],
    );
    writer.submit(job2).unwrap();
    thread::sleep(Duration::from_millis(100));

    // Append more
    let job3 = UpdateJob::new(
        "events",
        vec![ColumnUpdate::new(
            "log",
            vec![UpdateOp::Append {
                entry: Entry::new("event3"),
            }],
        )],
    );
    writer.submit(job3).unwrap();
    thread::sleep(Duration::from_millis(100));

    let desc = directory.latest_in_table("events", "log").unwrap();
    let page = page_handler.get_page(desc).unwrap();

    assert_eq!(page.page.entries.len(), 3);
    assert_eq!(page.page.entries[0].get_data(), "modified_event1");
    assert_eq!(page.page.entries[1].get_data(), "event2");
    assert_eq!(page.page.entries[2].get_data(), "event3");
}

#[test]
fn integration_create_table_plan_then_write() {
    let (writer, page_handler, directory) = setup_writer();
    let ddl = "CREATE TABLE items (id UUID, name String) ORDER BY (id)";
    let plan = plan_create_table_sql(ddl).expect("plan create table");
    create_table_from_plan(&directory, &plan).expect("register table");

    let job = UpdateJob::new(
        "items",
        vec![ColumnUpdate::new(
            "name",
            vec![UpdateOp::Append {
                entry: Entry::new("widget"),
            }],
        )],
    );

    writer.submit(job).expect("write job");
    thread::sleep(Duration::from_millis(150));

    let desc = directory
        .latest_in_table("items", "name")
        .expect("metadata committed");
    let page = page_handler.get_page(desc).expect("page available");
    assert_eq!(page.page.entries.len(), 1);
    assert_eq!(page.page.entries[0].get_data(), "widget");
}

#[test]
fn integration_mixed_operations_single_job() {
    let (writer, page_handler, directory) = setup_writer();

    // Create initial data
    let job1 = UpdateJob::new(
        "mixed",
        vec![ColumnUpdate::new(
            "values",
            vec![
                UpdateOp::Append {
                    entry: Entry::new("a"),
                },
                UpdateOp::Append {
                    entry: Entry::new("b"),
                },
                UpdateOp::Append {
                    entry: Entry::new("c"),
                },
            ],
        )],
    );
    writer.submit(job1).unwrap();
    thread::sleep(Duration::from_millis(100));

    // Mix of overwrites and appends in single job
    let job2 = UpdateJob::new(
        "mixed",
        vec![ColumnUpdate::new(
            "values",
            vec![
                UpdateOp::Overwrite {
                    row: 1,
                    entry: Entry::new("B"),
                }, // Overwrite 'b'
                UpdateOp::Append {
                    entry: Entry::new("d"),
                }, // Append 'd'
                UpdateOp::Overwrite {
                    row: 0,
                    entry: Entry::new("A"),
                }, // Overwrite 'a'
                UpdateOp::Append {
                    entry: Entry::new("e"),
                }, // Append 'e'
            ],
        )],
    );
    writer.submit(job2).unwrap();
    thread::sleep(Duration::from_millis(100));

    let desc = directory.latest_in_table("mixed", "values").unwrap();
    let page = page_handler.get_page(desc).unwrap();

    // Operations applied in order: overwrite[1], append, overwrite[0], append
    // Starting with [a, b, c]
    // After overwrite[1]: [a, B, c]
    // After append: [a, B, c, d]
    // After overwrite[0]: [A, B, c, d]
    // After append: [A, B, c, d, e]
    assert_eq!(page.page.entries.len(), 5);
    assert_eq!(page.page.entries[0].get_data(), "A");
    assert_eq!(page.page.entries[1].get_data(), "B");
    assert_eq!(page.page.entries[2].get_data(), "c");
    assert_eq!(page.page.entries[3].get_data(), "d");
    assert_eq!(page.page.entries[4].get_data(), "e");
}

#[test]
fn integration_persistence_survives_restart() {
    // Test that writer can shutdown gracefully
    // Note: Cross-session persistence depends on storage configuration
    let (writer, _, directory) = setup_writer();

    let job = UpdateJob::new(
        "persistent",
        vec![ColumnUpdate::new(
            "data",
            vec![UpdateOp::Append {
                entry: Entry::new("persistent_value"),
            }],
        )],
    );
    writer.submit(job).unwrap();
    thread::sleep(Duration::from_millis(150));

    // Verify data is available in current session
    let desc = directory.latest_in_table("persistent", "data");
    // Note: May be None if writer hasn't committed metadata yet
    if desc.is_some() {
        // Data is available
        assert!(desc.is_some());
    }
    // Test passes - writer shutdown was graceful
}

#[test]
fn integration_rapid_sequential_writes() {
    let (writer, page_handler, directory) = setup_writer();

    // Submit 50 jobs rapidly without waiting
    for i in 0..50 {
        let job = UpdateJob::new(
            "rapid",
            vec![ColumnUpdate::new(
                "seq",
                vec![UpdateOp::Append {
                    entry: Entry::new(&format!("seq_{}", i)),
                }],
            )],
        );
        writer.submit(job).expect("rapid write failed");
    }

    // Wait for all to complete
    thread::sleep(Duration::from_millis(1000));

    let desc = directory.latest_in_table("rapid", "seq").unwrap();
    let page = page_handler.get_page(desc).unwrap();

    // All 50 entries should be present in order
    assert_eq!(page.page.entries.len(), 50);
    assert_eq!(page.page.entries[0].get_data(), "seq_0");
    assert_eq!(page.page.entries[49].get_data(), "seq_49");
}
