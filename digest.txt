Directory structure:
└── satori/
    ├── readme.md
    ├── Cargo.toml
    ├── do.md
    ├── entry.md
    ├── innocent_file.txt
    ├── page.md
    ├── table_metadata_store.md 
    └── src/
        ├── compressor.rs
        ├── context.rs
        ├── entry.rs
        ├── latest.rs
        ├── main.rs
        ├── metadata_store.rs
        ├── ops_handler.rs
        ├── page.rs
        ├── page_cache.rs
        ├── page_io.rs
        └── page_updates.rs

================================================
FILE: readme.md
================================================
a distributed HTAP database that will mog everything that comes in its way

we are going to win


================================================
FILE: Cargo.toml
================================================
[package]
name = "idk_uwu_ig"
version = "0.1.0"
edition = "2024"

[dependencies]
bincode = "1.3"
lz4_flex = "0.11.5"
serde = { version = "1.0.219", features = ["derive"] }
serde_json = "1.0.143"



================================================
FILE: do.md
================================================
- add Entry structur types
- add Page structure types
- init a page and add some entry to it
- fetch a page


the page metadata would be aware of the row ranges of entries in it btw




================================================
FILE: entry.md
================================================
Entry:

{[prefix meta] -- [actual data] -- [suffix meta]}
ps: seriously dont remember why we need suffix meta btw, 
was it for reverse iteration ? its skipped in the implementation
for now


add something to a column ->
    check if that column exists from the table metadata ->
        if it doesnt, create a new page and insert the data in there
        if it does, find out from the table metadata itself in which page is the latest column entry kept

worry only about adding new entries to columns now, like a timeful append only KV store



create column -> add a new page at the end of file 
add something to a column -> find the current page for that column from the table metadata store and 


================================================
FILE: innocent_file.txt
================================================
hiihiihiihiihiihiihiihii


================================================
FILE: page.md
================================================
[Page Metadata]
[
    [Entry0]
    [Entry1]
    [Entry2]
    ..
    ..
]


================================================
FILE: table_metadata_store.md 
================================================
keep this in a separate file for now

what structure to store the table metadata in:

the columns:

col0 -> (page_x -> page_y -> page_z...)
...
...
...

map<String,Vector<u64>>


================================================
FILE: src/compressor.rs
================================================
use crate::{page_cache::CombinedCache, page_io::{read_from_path,write_to_path}};
use crate::metadata_store;
use lz4_flex::{compress_prepend_size, decompress_size_prepended};
use bincode;
use crate::page::Page;
use crate::page_cache::{PageCacheEntryUncompressed,PageCacheEntryCompressed};

/*
reads and writes stuff via page_io
and also deals with insertions in uncompressed page cache
*/
pub struct Compressor {}

impl Compressor {
    pub fn new() -> Self {
        Compressor {}
    }

    // this compresses some page from cache and flushes it btw
    // jsyk there are multiple race conditions with this shit btw
    pub fn compress(&self, tablemeta: metadata_store::TableMetaStore,mut cache: CombinedCache, id: &str) -> Option<bool>{
        // reads something from un-compressed page cache , compresses and write to disk 
        // also invalidates the compressed page cache
        let page = &cache.uncompressed_pages.get(id).unwrap().page.page;
        let raw = bincode::serialize(page).expect("serialize Page failed");
        let compressed = compress_prepend_size(&raw);

        let (path,offset) = tablemeta.get_page_path_and_offset(id).unwrap();
        let _ = write_to_path(path, offset, compressed);
        Some(true)
    }

    pub fn decompress(&self, tablemeta: &metadata_store::TableMetaStore ,mut cache: CombinedCache,id: &str) -> CombinedCache{
        let (path,offset) = tablemeta.get_page_path_and_offset(id).unwrap();
        let compressed_data = read_from_path(path, offset);
        let decompressed_data = decompress_size_prepended(&compressed_data).expect("decompress failed");
        cache.compressed_pages.add(id,PageCacheEntryCompressed {page: decompressed_data});
        cache
    }

    pub fn decompress_from_cache(&self, tablemeta: &metadata_store::TableMetaStore ,mut cache: CombinedCache,id: &str) -> CombinedCache{
        let compressed_data = &cache.compressed_pages.get(id).unwrap().page.page;
        let decompressed_data_raw = decompress_size_prepended(compressed_data).expect("decompress failed");
        let decompressed_data: Page = bincode::deserialize(&decompressed_data_raw).expect("deserialize failed");
        cache.uncompressed_pages.add(id, PageCacheEntryUncompressed {page: decompressed_data});
        cache
    }

    
}


================================================
FILE: src/context.rs
================================================
use crate::compressor::Compressor;
use crate::page_cache::CombinedCache;
use crate::page_io::IOHandler;

pub struct Context {
    pub cache: CombinedCache,
    pub io_handler: IOHandler,
    pub compressor: Compressor,
}


================================================
FILE: src/entry.rs
================================================
/*
It only matters for the start of the query/txn to the end of it
*/
use std::collections::VecDeque;
use std::time::{SystemTime, UNIX_EPOCH};
use serde::{Serialize, Deserialize};

pub fn current_epoch_millis() -> u64 {
   SystemTime::now()
       .duration_since(UNIX_EPOCH)
       .expect("Time went backwards")
       .as_millis() as u64
}

#[derive(Serialize, Deserialize)]
pub struct Entry {
    prefix_meta: String,
    data: String,
    suffix_meta: String,
}

impl Entry {
    pub fn new(data: &str) -> Self {
        Entry {
            prefix_meta: "".to_string(),
            data: data.to_string(),
            suffix_meta: "".to_string(),
        }
    }
}

struct Link {
    commit_time: u64,
    entry: Entry,
    locked_by: u64,
}

impl Link {
    fn new(entry: Entry) -> Self {
        Link { commit_time: 69696969, entry: entry, locked_by: 0 }
    }

    fn lock() {
        // increase the locked_by count in a threadsafe way
    }

    fn unlock() {
        // decrease count
    }
}

// stores and deals with the Entry Chains
// an Entry id is nothing but: (col_name::row_idx)
// Entry chains are nothing but: (col_name::row_idx::commit_time)
pub struct EntryKeeper{
    store: VecDeque<Link>
}

impl EntryKeeper{
    fn new() -> Self {
        EntryKeeper { store: VecDeque::new() }
    }

    // adds a new link to the chain
    fn add_link(&mut self, entry: Entry) {
        self.store.push_back(Link::new(entry));

        // we need update the latest state of this stuff without blocking any reads for it there
    }

    /*
    remove a link when:
    - there is another link right after it
    - when the time has passed so you can be certain with the fact that none of the upcoming queries would be considering it
    - the link isnt locked by any running query
    
    note that links are always sorted by created_at
    */

    // tries to remove the first link
    fn rem_link(&mut self) {
        if self.store.len() < 2 {
            return
        }
        if current_epoch_millis() >  self.store[0].commit_time && self.store[0].locked_by == 0 {
            self.store.pop_front();
        }
    }

}


================================================
FILE: src/latest.rs
================================================
// how the hell do I implement latest

/*
we NEED to store this contigiously and in-place modifications are ALLOWED
(how to handle page splits ?)

this is essentially just the latest state for each column and would be the first thing a query looks for when it comes

we essentially need to have that 

what the fuck man, cant really think

fuck thsi shit man, living this life is suffocating 
*/


================================================
FILE: src/main.rs
================================================
use std::collections::HashMap;
use std::fs::File;
use std::hash::Hash;
use std::io::{self, Seek, SeekFrom, Write};
mod entry;
mod page_io;
mod metadata_store;
mod ops_handler;
mod page_cache;
mod page;
mod context;
mod compressor;

// makes a new page and returns its offset
fn make_new_page_at_EOF() -> io::Result<(u64)>{
    // make a new page at EOF
    
    // return the offset
    Ok((69))
}


fn do_shit_to_file(data: &[u8], fd: &mut File) -> io::Result<String> {
    // append data to that file dumbly
    
    // seek to the bottom of the file
    fd.seek(SeekFrom::End(0))?;
    fd.write_all(data)?;

    // append data to it
    Ok("UwU".to_string())

}

fn main() -> io::Result<()> {
    println!("Hello, world!");

    let mut fd = File::options().read(true).write(true).open("./innocent_file.txt")?;
    let data = b"hii";

    print!("{}", do_shit_to_file(data,&mut fd)?);

    Ok(())
}

/*
just write stuff to a file

just make a function which does this:

takes a file descriptor
appends shit to the file it belongs to, that's it


*/

































================================================
FILE: src/metadata_store.rs
================================================
/*
this things keeps track of 'where on disk' the compressed pages of a certain table lies

we would also need to keep track of MVCC stuff here

lets kinda accept the fact that: 'contagious pages cant be kept together every single time' , atleast
not without giving away write performance and worst case massive disk movements

I think the best we can do for the columnar compressed pages updates is to just do the 'best effort' of just storing them durably(wherever we can on the disk at that time) when they come
and just round them up together(on disk) during compactions

okay, so what should the metadata store structure look like, it needs to:
    - keep track of where the compressed Pages are for a particular column
    - should support keeping track of multiple version of them, if there are a lot of versions of a certain page, we must prioritize that
    after compaction - atleast the latest versions of pages are physically kept close sequentially


currently we store stuff as:

# Table metadata store

M[col_name] -> [(),()...]
                 |
                 ~->{start_row_idx,end_row_idx,PageMetadata: [(),()...]}
                                                              | // we store multiple versions of Pages for MVCC stuff
                                                              ~-> {id,locked_by_cnt,commit_time,disk_path,offset} 

we should also do:
M[page_id] -> I mean, we should just do this shit like: {id,locked_by_cnt,commit_time,disk_path,offset}

and just keep page_id like a foreign key in M[col_name] shit instead of keeping it all there, would also be faster to just get it in just O(x)
once than to go through a lot of O(x) + O(x).... nested stuff every single time
*/
use std::collections::HashMap;
use std::sync::{Arc,RwLock};

use crate::entry::{current_epoch_millis, Entry};
use crate::page::{self, Page};
use crate::context::Context;
use crate::page_cache::CombinedCache;


// this will be immutable throughout its lifetime btw
pub struct PageMetadata {
    pub id: String, // this is page id btw
    pub disk_path: String,
    pub offset: u64, // where to find the compressed page in that path
}

pub struct MVCCKeeperEntry {
    pub page_id: String,
    pub locked_by: u8, // this needs to be atomic counter btw, todo I guess
    pub commit_time: u64,
}

pub struct TableMetaStoreEntry {
    pub start_idx: u64,
    pub end_idx: u64,
    pub page_metas: Vec<MVCCKeeperEntry>
}

pub struct RangeScanMetaResponse{
    page_metas: Vec<PageMetadata>
}

pub struct TableMetaStore {
    /* 

    M[page_id]  ->                      |~~>(disk_path,offset)
                                        |
    M[col_name] -> [                    |
                                        | 
                        (l0,r0) -> [Page_id_at_x < Page_id_at_y < Page_id_at_z....],
                        (l1,r1) -> ...
                        (l2,r2)
                        (l3,r3)
                        ..
                        ..
                    ] 
    
    */
    col_data: HashMap<String,Arc<RwLock<Vec<TableMetaStoreEntry>>>>, // this just keeps the page_id
    page_data: HashMap<String,Arc<PageMetadata>> // this keeps the actual page metadata, and owns the ARCs
}

impl PageMetadata {
    // also returns an id maybe ??
    fn new(disk_path: String,offset: u64) -> Self {
        Self {
            id: "1111111".to_string(), // todo: use a real rand id gen here good ser
            disk_path: disk_path,
            offset: offset,
        }
    }
}

impl MVCCKeeperEntry {
    fn new(id: String) -> Self {
        Self {
            page_id: id,
            commit_time: current_epoch_millis(),
            locked_by: 0,
        }
    }
}

impl Drop for PageMetadata {
    fn drop(&mut self) {
        // so we do a bunch of stuff here

        let me = self;

        // so if a page meta is being dropped, we don't want to:
        // - have it on disk
        // - have it in any cache(both compressed and uncompressed)
        // so yeah, todo I guess AHAHAHHAH
    }
}


impl TableMetaStoreEntry {
    fn new(start_idx: u64, end_idx: u64) -> Self {
        Self {
            start_idx: start_idx, end_idx: end_idx, page_metas: vec![]
        }
    }

    fn copied(&self) {
        // returns a copy ??? hoe ?
        // TODO
    }
}


impl TableMetaStore {
    fn new() -> Self {
        Self {
            col_data: HashMap::new(),
            page_data: HashMap::new()
        }
    }

    pub fn get_page_path_and_offset(&self, id: &str) -> Option<(String,u64)>{
        let entry: &PageMetadata = self.page_data.get(id).unwrap();
        let path = &entry.disk_path;
        let offset = &entry.offset;
        
        Some((path.to_string(),*offset))
    }

    pub fn get_latest_page_meta(&self, column: &str) -> Option<&Arc<PageMetadata>> {
        self.col_data
            .get(column)?
            .read()
            .ok()?
            .last()?
            .page_metas
            .last()
            .and_then(|mvcc_entry| self.page_data.get(&mvcc_entry.page_id))
    }

    // returns the new generated page id
    fn add_new_page_meta(&mut self,disk_path: String, offset: u64) -> String {
        let meta_object = PageMetadata::new(disk_path,offset);
        let id = meta_object.id.clone();
        self.page_data.insert(id.clone(),Arc::new(meta_object));
        // now meta object is stored in clouds meta store AHAHAH, no ownership bullos from here, will be freed whenever it gets freed by the laws of physics and inevitability of flowing time
        id
    }

    fn add_new_page_to_col(&mut self, col: String, disk_path: String, offset: u64) {
        let page_id = self.add_new_page_meta(disk_path, offset);
        
        if !self.col_data.contains_key(&col) {
            self.col_data.insert(col.clone(), Arc::new(RwLock::new(Vec::new())));
        }
        
        let mut col_guard = self.col_data.get(&col).unwrap().write().unwrap();
        
        if col_guard.is_empty() {
            col_guard.push(TableMetaStoreEntry {
                start_idx: 0,
                end_idx: 1,
                page_metas: vec![MVCCKeeperEntry::new(page_id)],
            });
        } else {
            let last_entry = col_guard.last_mut().unwrap();
            last_entry.end_idx += 1;
            last_entry.page_metas.push(MVCCKeeperEntry::new(page_id));
        }
    }

    fn get_ranged_pages_meta(col: String, l: u8,r: u8) {
        
    }
    
}

// // how the hell do I get the PageCache context here lmao
// fn append_to_column(context: Context, tableMetaStore: TableMetaStore, column: &str, data: &str) -> Option<()>{
//     // find out the current page from table meta store
//     let latest_page_meta = tableMetaStore.get_latest_page_meta(column).unwrap().page_metas.last().unwrap();


//     // most probably need to add some abstraction for below stuff

//     if context.cache.uncompressed_pages.has(&latest_page_meta) {
//         // todo
//         // update
//     } else if context.cache.compressed_pages.has(&latest_page_meta){
//         // todo
//         // decompress page
//         // pull into uncompressed pages
//         // update
//     } else {
//         // do IO shit
//         // pull into compressed pages
//         // pull into decompressed pages
//     }

//     if true {
//         let new_page = Page::new();

//         // creating a new page
//         // add an empty entry
//     }

//     None
// }



================================================
FILE: src/ops_handler.rs
================================================
use crate::entry::Entry;
use crate::metadata_store::TableMetaStore;
use crate::page_cache::CombinedCache;
use crate::compressor::Compressor;


// TODO: we also have to update the (l,r) ranges whenever we upsert something into it and there certainly has to be a better way to do it along with updating metadata in one shot
fn upsert_data_into_column(entry: Entry, compressor: Compressor, meta_store: TableMetaStore, mut cache: CombinedCache, col: &str,data: &str) -> Result<bool, Box<dyn std::error::Error>> {
    let latest_page_meta = meta_store.get_latest_page_meta(col).unwrap();
    let page_id = latest_page_meta.id.clone();
    let entry = Entry::new(data);

    // check if page in uncompressed cache, if yes, just update and flush 
    if cache.uncompressed_pages.has(&latest_page_meta.id) {
        cache.uncompressed_pages.get(&page_id).unwrap().page.page.add_entry(entry);
        // note tha the below stuff might totally own the cache and drain it once gone out of scope
        let ok = compressor.compress(meta_store,cache,&page_id).unwrap();
        return Ok((ok))
    }
    
    // check if page in compressed cache, if yes, decompress and put into compressed cache and do the above
    if cache.compressed_pages.has(&latest_page_meta.id) {
        cache = compressor.decompress_from_cache(&meta_store,cache,&page_id);
        // fetches from compressed cache ,decompresses, inserts in the uncompressed page cache, then does the sames as above
        cache.uncompressed_pages.get(&page_id).unwrap().page.page.add_entry(entry);
        let ok = compressor.compress(meta_store,cache,&page_id).unwrap();
        return Ok((ok))
    }

    // fetches from disk, then does the same as above
    cache = compressor.decompress(&meta_store,cache,&page_id);
    cache = compressor.decompress_from_cache(&meta_store,cache,&page_id);
    cache.uncompressed_pages.get(&page_id).unwrap().page.page.add_entry(entry);
    let ok = compressor.compress(meta_store,cache,&page_id).unwrap();

    Ok((ok))


}

// I hope the below stuff is correct, im fried rn
fn update_column_entry(entry: Entry, compressor: Compressor, meta_store: TableMetaStore, mut cache: CombinedCache, col: &str,data: &str, row: u64) -> Result<bool, Box<dyn std::error::Error>> {
    let latest_page_meta = meta_store.get_latest_page_meta(col).unwrap();
    let page_id = latest_page_meta.id.clone();
    let entry = Entry::new(data);

    // check if page in uncompressed cache, if yes, just update and flush 
    if cache.uncompressed_pages.has(&latest_page_meta.id) {
        cache.uncompressed_pages.get(&page_id).unwrap().page.page.entries[row as usize] = entry;
        // note tha the below stuff might totally own the cache and drain it once gone out of scope
        let ok = compressor.compress(meta_store,cache,&page_id).unwrap();
        return Ok((ok))
    }
    
    // check if page in compressed cache, if yes, decompress and put into compressed cache and do the above
    if cache.compressed_pages.has(&latest_page_meta.id) {
        cache = compressor.decompress_from_cache(&meta_store,cache,&page_id);
        // fetches from compressed cache ,decompresses, inserts in the uncompressed page cache, then does the sames as above
        cache.uncompressed_pages.get(&page_id).unwrap().page.page.entries[row as usize] = entry;
        let ok = compressor.compress(meta_store,cache,&page_id).unwrap();
        return Ok((ok))
    }

    // fetches from disk, then does the same as above
    cache = compressor.decompress(&meta_store,cache,&page_id);
    cache = compressor.decompress_from_cache(&meta_store,cache,&page_id);
    // cache.uncompressed_pages.get(&page_id).unwrap().page.page.add_entry(entry);
    cache.uncompressed_pages.get(&page_id).unwrap().page.page.entries[row as usize] = entry;
    let ok = compressor.compress(meta_store,cache,&page_id).unwrap();

    Ok((ok))


}

fn read_single_column_entry(col: String, row: u64) {

}

fn range_scan_column_entry(col: String, l_row: u64,r_row: u64) {
    /*
    quickly find the internal (l,r) groupings and the latest pages needed for it and put an lock on them so they dont perish
    

    query them parallely fast 

    holy fuck, we need some sort of lock thingy per column lol

    yep, something... like, the thing is , our (l,r) keeper thingy isnt really MVCCing and I dont really want it to be that way

    once we know that "nothing will change in between me figuring out the bounds", we can figure things out with one binary search

    soo... figure out the bounds, and get their latest page IDs 

    okay, so thinking about it, our Pages are kinda versioned nicely because yesterday's nubskr was kind to us

    so we don't need to worry about that... 

    so.. just figure out how to get those (l,r) bounds in one shot and call it a day

    also note that you cant just lock stuff as that would involve "waiting" into the game which we dont want


    okay, so for now we just take a read lock and get an immutable snapshot of the data, can we do that ??

    umm... just get a read lock, run a fast binary search, get the index bounds of stuff you need and just grab the latest page metas of them, that's fucking it

    I mean, we can kinda just.... clone after taking a read lock and never have to do a call after that ever again for this request

    okay, so we kinda need a very very fast way to get the latest page metas for a contigious group of (l,r) bounds

    okay, whateer, lets keep it simple, make all that internal stuff be done my metadata store, and it would return us with just relevant page_ids

    and after that we would be done with it and just deal with page cache store to actually get the stuffs
    */
    

}




================================================
FILE: src/page.rs
================================================
use crate::entry;
use serde::{Serialize, Deserialize};

#[derive(Serialize, Deserialize)]
pub struct Page {
    pub page_metadata: String,
    pub entries: Vec<entry::Entry>,
}

impl Page {
    pub fn new() -> Self {
        Self {
            page_metadata: "".to_string(),
            entries: vec![],
        }
    }

    pub fn add_entry(&mut self, entry: entry::Entry) {
        // what does it means to add an entry in a page
        self.entries.push(entry);
        

        // now its done in memory, we need to do it on disk as well
    }
}



================================================
FILE: src/page_cache.rs
================================================
use std::collections::BTreeSet;
use std::collections::HashMap;
use crate::page;
use crate::entry::current_epoch_millis;
// user space page cache

// a dequeue of Pages(which are nothing but a set of entries)
// fixed sized because of limits 
// we should be able to access a page from its ID or something 

/*

set with (used_id,id)
---

store[id] -> PageCacheEntry
Set((used_time,id),()...)

create when adding, remove when removing
*/
const LRUsize:usize = 10;

pub struct PageCacheEntry<T> {
    pub page: T,
    pub used_time: u64
}

pub struct PageCacheEntryUncompressed {
    pub page: page::Page,
}

pub struct PageCacheEntryCompressed {
    pub page: Vec<u8>, // a bunch of raw bytes that we read from the disk
}


pub struct PageCache<T>{
    pub store: HashMap<String,PageCacheEntry<T>>,
    pub lru_queue: BTreeSet<(u64,String)>
}

impl<T> PageCache<T> {
    pub fn new() -> Self {
        PageCache{ store: HashMap::new() , lru_queue: BTreeSet::new()}
    }

    // adds a certain Page to mem
    pub fn add(&mut self,id: &str, page: T) {
        // check if an entry already exists
        if self.store.contains_key(id) {
            // remove it from set
            let entry = self.store.get(id);
            // let wut = entry.unwrap().used_time;
            self.lru_queue.remove(&((entry.unwrap()).used_time,String::from(id)));
        }

        // make an new entry
        let used_time = current_epoch_millis();
        
        let entry = PageCacheEntry{page: page,used_time: used_time};

        self.store.insert(id.to_string(),entry);

        self.lru_queue.insert((used_time,id.to_string()));
        if self.lru_queue.len() > LRUsize {
            let (oldest_time, oldest_id) = self.lru_queue.iter().next().unwrap().clone();
            self.evict(&oldest_id);
        }
    }

    pub fn has(&self, id: &str) -> bool {
        self.store.contains_key(id)
    }

    // so this returns a reference to the entry
    pub fn get(&mut self, id: &str) -> Option<&mut PageCacheEntry<T>> {
        // this is more complex btw

        // we need to send a mutable reference here btw

        self.store.get_mut(id)
    }

    pub fn evict(&mut self, id: &str) {
        // remove from lru_queue
        self.lru_queue.remove(&(self.store.get(id).unwrap().used_time, id.to_string()));
        self.store.remove(id);
    }

}

pub struct CombinedCache {
    pub compressed_pages: PageCache<PageCacheEntryCompressed>,
    pub uncompressed_pages: PageCache<PageCacheEntryUncompressed>,
}

impl CombinedCache {
    pub fn new() -> Self {
        Self {compressed_pages: PageCache::new() , uncompressed_pages: PageCache::new()}
    }
}


================================================
FILE: src/page_io.rs
================================================
use std::io;
use std::fs::File;
use std::io::{Read, Seek, SeekFrom, Write};
use serde::Deserialize;
use serde::Serialize;

/*
so on disk, our Entries are just compressed pages


we use atmax 1mb compressed pages

we never get kernel optmizations involved for IO, we have our own user space page cache
*/

pub struct IOHandler {}

const PREFIX_META_SIZE: usize = 2048;

// impl IOHandler {
//     fn new() -> Self {
//         IOHandler {}
//     }

//     fn read_from_path(&self, path: String, offset: u64) -> Vec<u8> {
//         // Read data from the specified path and offset and returns raw bytes
//         let fd = File::open(path);
//         fd.seek(SeekFrom::Start(offset));

//         // read the prefix meta from the offset, get the exact size to read from the offset, then read that whole thing and return
//         let mut buffer = vec![0; PREFIX_META_SIZE as usize];
//         fd.read_exact(&mut buffer);
//         buffer
//     }

//     fn write_to_path(&self, path: String, offset: u64, data: Vec<u8>) -> bool {
//         // just dumbly write to that path and offset
//         let fd = File::create(path);
//         fd.seek(SeekFrom::Start(offset));
//         fd.write_all(&data);
//         true
//     }
// }

/*
this thing is responsible for flushing and fetching pages from disk
*/

#[derive(Deserialize)]
#[derive(Serialize)]
struct Metadata {
    read_size: usize
}

fn deserialize_metadata(buffer: Vec<u8>) -> Metadata {
    // Deserialize the buffer into a Metadata struct
    let json = String::from_utf8_lossy(&buffer);
    serde_json::from_str(&json).unwrap()
}

pub fn read_from_path(path: String, offset: u64) -> Vec<u8> {
    // Read data from the specified path and offset and returns raw bytes

    // we are opening new FDs here btw, try to keep an FD pool and stuff...
    let mut fd = File::open(path).unwrap();
    fd.seek(SeekFrom::Start(offset)).unwrap();

    // read the prefix meta from the offset, get the exact size to read from the offset, then read that whole thing and return
    let mut buffer = vec![0; PREFIX_META_SIZE as usize];
    fd.read_exact(&mut buffer).unwrap();

    let new_offset = offset + PREFIX_META_SIZE as u64;
     fd.seek(SeekFrom::Start(new_offset)).unwrap();
    let meta = deserialize_metadata(buffer);
    let actual_entry_size = meta.read_size;

    let mut ret_buffer = vec![0; actual_entry_size as usize];
    fd.read_exact(&mut ret_buffer).unwrap();

    ret_buffer
    // now, that buffer is just the raw bytes containing the serialized metadata for the entry
    // we need to deserialize it and read it, for now, let's just keep it human readable json
}

pub fn write_to_path(path: String, offset: u64, data: Vec<u8>) -> Result<(), io::Error> {
    let mut fd = File::create(path)?;
    fd.seek(SeekFrom::Start(offset))?;
    
    // Create metadata
    let data_size = data.len();
    let new_meta = Metadata { read_size: data_size };
    
    // Serialize and pad metadata to exactly PREFIX_META_SIZE
    let meta_json = serde_json::to_string(&new_meta).unwrap();
    let mut meta_buffer = vec![0u8; PREFIX_META_SIZE as usize];
    let meta_bytes = meta_json.as_bytes();
    meta_buffer[..meta_bytes.len()].copy_from_slice(meta_bytes);
    
    // Combine metadata + data in memory
    let mut combined = Vec::with_capacity(PREFIX_META_SIZE as usize + data.len());
    combined.extend_from_slice(&meta_buffer);
    combined.extend_from_slice(&data);
    
    // ONE write syscall for everything
    fd.write_all(&combined)?;
    
    Ok(())
}


================================================
FILE: src/page_updates.rs
================================================
/*
how the hell do we apply updates to a page

prereqs:
- it needs to be in cache
- the update patch needs to be available, how ??
    - what if the patch is simply bigger, 
        - like, can we somemhow manage a bit uneven size ? 



so what are the things like right now ?

first of all, cache heirarchies need to be present

like, if we have a page decompressed in PageCache , when it needs to be evicted, we would need to evict all its entries:
like: a Page is one thing, its entries is(are) multiple things

the moment a page is decompressed, we need to evict its compressed version from the `Compressed Page Cache`

Page Cache:
[Compressed Page Cache]
[Decompressed Page Cache]

Any cache in general should be:
- able to quickly check if a Page(whether compressed or uncompressed) is in memory or not

we can kinda use compressed Page references with the metadata store
*/

