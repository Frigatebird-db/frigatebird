Directory structure:
└── satori/
    ├── readme.md
    ├── Cargo.toml
    ├── do.md
    ├── entry.md
    ├── innocent_file.txt
    ├── page.md
    ├── table_metadata_store.md 
    └── src/
        ├── compressor.rs
        ├── context.rs
        ├── entry.rs
        ├── latest.rs
        ├── main.rs
        ├── metadata_store.rs
        ├── ops_handler.rs
        ├── page.rs
        ├── page_cache.rs
        ├── page_io.rs
        └── page_updates.rs

================================================
FILE: readme.md
================================================
a distributed HTAP database that will mog everything that comes in its way

we are going to win


================================================
FILE: Cargo.toml
================================================
[package]
name = "idk_uwu_ig"
version = "0.1.0"
edition = "2024"

[dependencies]
bincode = "1.3"
lz4_flex = "0.11.5"
serde = { version = "1.0.219", features = ["derive"] }
serde_json = "1.0.143"



================================================
FILE: do.md
================================================
- add Entry structur types
- add Page structure types
- init a page and add some entry to it
- fetch a page


the page metadata would be aware of the row ranges of entries in it btw




================================================
FILE: entry.md
================================================
Entry:

{[prefix meta] -- [actual data] -- [suffix meta]}
ps: seriously dont remember why we need suffix meta btw, 
was it for reverse iteration ? its skipped in the implementation
for now


add something to a column ->
    check if that column exists from the table metadata ->
        if it doesnt, create a new page and insert the data in there
        if it does, find out from the table metadata itself in which page is the latest column entry kept

worry only about adding new entries to columns now, like a timeful append only KV store



create column -> add a new page at the end of file 
add something to a column -> find the current page for that column from the table metadata store and 


================================================
FILE: innocent_file.txt
================================================
hiihiihiihiihiihiihiihii


================================================
FILE: page.md
================================================
[Page Metadata]
[
    [Entry0]
    [Entry1]
    [Entry2]
    ..
    ..
]


================================================
FILE: table_metadata_store.md 
================================================
keep this in a separate file for now

what structure to store the table metadata in:

the columns:

col0 -> (page_x -> page_y -> page_z...)
...
...
...

map<String,Vector<u64>>


================================================
FILE: src/compressor.rs
================================================
use crate::{page_cache::CombinedCache, page_io::{read_from_path,write_to_path}};
use crate::metadata_store;
use lz4_flex::{compress_prepend_size, decompress_size_prepended};
use bincode;

/*
reads and writes stuff via page_io
and also deals with insertions in uncompressed page cache
*/
struct Compressor {}

impl Compressor {
    fn new() -> Self {
        Compressor {}
    }

    // jsyk there are multiple race conditions with this shit btw
    fn compress(tablemeta: metadata_store::TableMetaStore  ,cache: CombinedCache, id: &str) -> Option<bool>{
        // reads something from un-compressed page cache , compresses and write to disk 
        // also invalidates the compressed page cache
        let page = &cache.uncompressed_pages.get(id).unwrap().page.page;
        let raw = bincode::serialize(page).expect("serialize Page failed");
        let compressed = compress_prepend_size(&raw);

        let (path,offset) = tablemeta.get_page_path_and_offset(id).unwrap();
        // todo: get the path and offset from table metadata store
        let _ = write_to_path(path, offset, compressed);
        Some(true)
    }

    fn decompress(tablemeta: metadata_store::TableMetaStore ,cache: CombinedCache,id: &str) -> Vec<u8> {
        // read from this path, decompress and stores those decompressed bytes into uncompressed page cache
        // todo: get the path and offset from table metadata store
        let (path,offset) = tablemeta.get_page_path_and_offset(id).unwrap();
        let compressed_data = read_from_path(path, offset);
        let decompressed_data = decompress_size_prepended(&compressed_data).expect("decompress failed");
        // todo: instead of returning, deserialize the decompressed_data and insert it into compressed_data
        decompressed_data
    }
}


================================================
FILE: src/context.rs
================================================
use crate::page_cache::CombinedCache;
use crate::page_io::IOHandler;

pub struct Context {
    pub cache: CombinedCache,
    pub io_handler: IOHandler,
}


================================================
FILE: src/entry.rs
================================================
/*
It only matters for the start of the query/txn to the end of it
*/
use std::collections::VecDeque;
use std::time::{SystemTime, UNIX_EPOCH};
use serde::{Serialize, Deserialize};

pub fn current_epoch_millis() -> u64 {
   SystemTime::now()
       .duration_since(UNIX_EPOCH)
       .expect("Time went backwards")
       .as_millis() as u64
}

#[derive(Serialize, Deserialize)]
pub struct Entry {
    prefix_meta: String,
    data: String,
    suffix_meta: String,
}

impl Entry {
    fn new() -> Self {
        Entry {
            prefix_meta: "".to_string(),
            data: "dummy".to_string(),
            suffix_meta: "".to_string(),
        }
    }
}

struct Link {
    commit_time: u64,
    entry: Entry,
    locked_by: u64,
}

impl Link {
    fn new(entry: Entry) -> Self {
        Link { commit_time: 69696969, entry: entry, locked_by: 0 }
    }

    fn lock() {
        // increase the locked_by count in a threadsafe way
    }

    fn unlock() {
        // decrease count
    }
}

// stores and deals with the Entry Chains
// an Entry id is nothing but: (col_name::row_idx)
// Entry chains are nothing but: (col_name::row_idx::commit_time)
pub struct EntryKeeper{
    store: VecDeque<Link>
}

impl EntryKeeper{
    fn new() -> Self {
        EntryKeeper { store: VecDeque::new() }
    }

    // adds a new link to the chain
    fn add_link(&mut self, entry: Entry) {
        self.store.push_back(Link::new(entry));

        // we need update the latest state of this stuff without blocking any reads for it there
    }

    /*
    remove a link when:
    - there is another link right after it
    - when the time has passed so you can be certain with the fact that none of the upcoming queries would be considering it
    - the link isnt locked by any running query
    
    note that links are always sorted by created_at
    */

    // tries to remove the first link
    fn rem_link(&mut self) {
        if self.store.len() < 2 {
            return
        }
        if current_epoch_millis() >  self.store[0].commit_time && self.store[0].locked_by == 0 {
            self.store.pop_front();
        }
    }

}


================================================
FILE: src/latest.rs
================================================
// how the hell do I implement latest

/*
we NEED to store this contigiously and in-place modifications are ALLOWED
(how to handle page splits ?)

this is essentially just the latest state for each column and would be the first thing a query looks for when it comes

we essentially need to have that 

what the fuck man, cant really think

fuck thsi shit man, living this life is suffocating 
*/


================================================
FILE: src/main.rs
================================================
use std::collections::HashMap;
use std::fs::File;
use std::hash::Hash;
use std::io::{self, Seek, SeekFrom, Write};
mod entry;
mod page_io;
mod metadata_store;
mod ops_handler;
mod page_cache;
mod page;
mod context;
mod compressor;

// makes a new page and returns its offset
fn make_new_page_at_EOF() -> io::Result<(u64)>{
    // make a new page at EOF
    
    // return the offset
    Ok((69))
}


fn do_shit_to_file(data: &[u8], fd: &mut File) -> io::Result<String> {
    // append data to that file dumbly
    
    // seek to the bottom of the file
    fd.seek(SeekFrom::End(0))?;
    fd.write_all(data)?;

    // append data to it
    Ok("UwU".to_string())

}

fn main() -> io::Result<()> {
    println!("Hello, world!");

    let mut fd = File::options().read(true).write(true).open("./innocent_file.txt")?;
    let data = b"hii";

    print!("{}", do_shit_to_file(data,&mut fd)?);

    Ok(())
}

/*
just write stuff to a file

just make a function which does this:

takes a file descriptor
appends shit to the file it belongs to, that's it


*/

































================================================
FILE: src/metadata_store.rs
================================================
/*
this things keeps track of 'where on disk' the compressed pages of a certain table lies

we would also need to keep track of MVCC stuff here

lets kinda accept the fact that: 'contagious pages cant be kept together every single time' , atleast
not without giving away write performance and worst case massive disk movements

I think the best we can do for the columnar compressed pages updates is to just do the 'best effort' of just storing them durably(wherever we can on the disk at that time) when they come
and just round them up together(on disk) during compactions

okay, so what should the metadata store structure look like, it needs to:
    - keep track of where the compressed Pages are for a particular column
    - should support keeping track of multiple version of them, if there are a lot of versions of a certain page, we must prioritize that
    after compaction - atleast the latest versions of pages are physically kept close sequentially


currently we store stuff as:

# Table metadata store

M[col_name] -> [(),()...]
                 |
                 ~->{start_row_idx,end_row_idx,PageMetadata: [(),()...]}
                                                              | // we store multiple versions of Pages for MVCC stuff
                                                              ~-> {id,locked_by_cnt,commit_time,disk_path,offset} 

we should also do:
M[page_id] -> I mean, we should just do this shit like: {id,locked_by_cnt,commit_time,disk_path,offset}

and just keep page_id like a foreign key in M[col_name] shit instead of keeping it all there, would also be faster to just get it in just O(x)
once than to go through a lot of O(x) + O(x).... nested stuff every single time
*/
use std::collections::HashMap;

use crate::page::Page;
use crate::context::Context;
use crate::page_cache::CombinedCache;

struct PageMetadata {
    id: String, // this is page id btw
    locked_by: u8,
    commit_time: u64, // when it came
    disk_path: String,
    offset: u64, // where to find the compressed page in that path
}

struct TableMetaStoreEntry {
    start_idx: u64,
    end_idx: u64,
    page_metas: Vec<PageMetadata> // todo: change this shit to just Vec<String> as we are storing page metadata separately in meta store now
}

pub struct TableMetaStore {
    // M[col_name] -> [(),()..]
    col_data: HashMap<String,Vec<TableMetaStoreEntry>>,
    page_data: HashMap<String,PageMetadata>
}

impl TableMetaStoreEntry {
    fn new(start_idx: u64, end_idx: u64) -> Self {
        Self {
            start_idx: start_idx, end_idx: end_idx, page_metas: vec![]
        }
    }

    fn copied(&self) {
        // returns a copy ??? hoe ?
        // TODO
    }
}


impl TableMetaStore {
    fn new() -> Self {
        Self {
            col_data: HashMap::new(),
            page_data: HashMap::new()
        }
    }


    pub fn get_page_path_and_offset(&self, id: &str) -> Option<(String,u64)>{
        let entry = self.page_data.get(id).unwrap();
        let path = &entry.disk_path;
        let offset = &entry.offset;
        
        Some((path.to_string(),*offset))
    }

    fn get_latest_page_meta(&self, column: &str) -> Option<&TableMetaStoreEntry> {
        self.col_data.get(column)?.last()
    }
}

// how the hell do I get the PageCache context here lmao
fn append_to_column(context: Context, tableMetaStore: TableMetaStore, column: &str, data: &str) -> Option<()>{
    // find out the current page from table meta store
    let latest_page_meta = tableMetaStore.get_latest_page_meta(column).unwrap().page_metas.last().unwrap();


    // most probably need to add some abstraction for below stuff

    if context.cache.uncompressed_pages.has(&latest_page_meta.id) {
        // todo
        // update
    } else if context.cache.compressed_pages.has(&latest_page_meta.id){
        // todo
        // decompress page
        // pull into uncompressed pages
        // update
    } else {
        // do IO shit
        // pull into compressed pages
        // pull into decompressed pages
    }

    if true {
        let new_page = Page::new();

        // creating a new page
        // add an empty entry
    }

    None
}



================================================
FILE: src/ops_handler.rs
================================================
use crate::entry;

fn upsert_entry_into_column(col: String,entry: entry::Entry) {
    // get the col meta

    // go from there

    /* so we need to get the column meta first,

    then get the latest page maybe

    check its size fast, if its already too big then just make a new one if not then do a lot of slow stuff lol
    */
}


================================================
FILE: src/page.rs
================================================
use crate::entry;
use serde::{Serialize, Deserialize};

#[derive(Serialize, Deserialize)]
pub struct Page {
    page_metadata: String,
    entries: Vec<entry::Entry>,
}

impl Page {
    pub fn new() -> Self {
        Self {
            page_metadata: "".to_string(),
            entries: vec![],
        }
    }

    pub fn add_entry(&mut self, entry: entry::Entry) {
        // what does it means to add an entry in a page
        self.entries.push(entry);

        // now its done in memory, we need to do it on disk as well
    }
}



================================================
FILE: src/page_cache.rs
================================================
use std::collections::BTreeSet;
use std::collections::HashMap;
use crate::page;
use crate::entry::current_epoch_millis;
// user space page cache

// a dequeue of Pages(which are nothing but a set of entries)
// fixed sized because of limits 
// we should be able to access a page from its ID or something 

/*

set with (used_id,id)
---

store[id] -> PageCacheEntry
Set((used_time,id),()...)

create when adding, remove when removing
*/
const LRUsize:usize = 10;

pub struct PageCacheEntry<T> {
    pub page: T,
    pub used_time: u64
}

pub struct PageCacheEntryUncompressed {
    pub page: page::Page,
}

pub struct PageCacheEntryCompressed {
    pub page: Vec<u8>, // a bunch of raw bytes that we read from the disk
}


pub struct PageCache<T>{
    pub store: HashMap<String,PageCacheEntry<T>>,
    pub lru_queue: BTreeSet<(u64,String)>
}

impl<T> PageCache<T> {
    pub fn new() -> Self {
        PageCache{ store: HashMap::new() , lru_queue: BTreeSet::new()}
    }

    // adds a certain Page to mem
    pub fn add(&mut self,id: &str, page: T) {
        // check if an entry already exists
        if self.store.contains_key(id) {
            // remove it from set
            let entry = self.store.get(id);
            // let wut = entry.unwrap().used_time;
            self.lru_queue.remove(&((entry.unwrap()).used_time,String::from(id)));
        }

        // make an new entry
        let used_time = current_epoch_millis();
        
        let entry = PageCacheEntry{page: page,used_time: used_time};

        self.store.insert(id.to_string(),entry);

        self.lru_queue.insert((used_time,id.to_string()));
        if self.lru_queue.len() > LRUsize {
            let (oldest_time, oldest_id) = self.lru_queue.iter().next().unwrap().clone();
            self.evict(&oldest_id);
        }
    }

    pub fn has(&self, id: &str) -> bool {
        self.store.contains_key(id)
    }

    // so this returns a reference to the entry
    pub fn get(&self, id: &str) -> Option<&PageCacheEntry<T>> {
        // this is more complex btw
        self.store.get(id)
    }

    pub fn evict(&mut self, id: &str) {
        // remove from lru_queue
        self.lru_queue.remove(&(self.store.get(id).unwrap().used_time, id.to_string()));
        self.store.remove(id);
    }

}

pub struct CombinedCache {
    pub compressed_pages: PageCache<PageCacheEntryCompressed>,
    pub uncompressed_pages: PageCache<PageCacheEntryUncompressed>,
}

impl CombinedCache {
    pub fn new() -> Self {
        Self {compressed_pages: PageCache::new() , uncompressed_pages: PageCache::new()}
    }
}


================================================
FILE: src/page_io.rs
================================================
use std::io;
use std::fs::File;
use std::io::{Read, Seek, SeekFrom, Write};
use serde::Deserialize;
use serde::Serialize;

/*
so on disk, our Entries are just compressed pages

an entry is nothing but
*/

pub struct IOHandler {}

const PREFIX_META_SIZE: usize = 4096;

// impl IOHandler {
//     fn new() -> Self {
//         IOHandler {}
//     }

//     fn read_from_path(&self, path: String, offset: u64) -> Vec<u8> {
//         // Read data from the specified path and offset and returns raw bytes
//         let fd = File::open(path);
//         fd.seek(SeekFrom::Start(offset));

//         // read the prefix meta from the offset, get the exact size to read from the offset, then read that whole thing and return
//         let mut buffer = vec![0; PREFIX_META_SIZE as usize];
//         fd.read_exact(&mut buffer);
//         buffer
//     }

//     fn write_to_path(&self, path: String, offset: u64, data: Vec<u8>) -> bool {
//         // just dumbly write to that path and offset
//         let fd = File::create(path);
//         fd.seek(SeekFrom::Start(offset));
//         fd.write_all(&data);
//         true
//     }
// }

/*
this thing is responsible for flushing and fetching pages from disk
*/

#[derive(Deserialize)]
#[derive(Serialize)]
struct Metadata {
    read_size: usize
}

fn deserialize_metadata(buffer: Vec<u8>) -> Metadata {
    // Deserialize the buffer into a Metadata struct
    let json = String::from_utf8_lossy(&buffer);
    serde_json::from_str(&json).unwrap()
}

pub fn read_from_path(path: String, offset: u64) -> Vec<u8> {
    // Read data from the specified path and offset and returns raw bytes
    let mut fd = File::open(path).unwrap();
    fd.seek(SeekFrom::Start(offset)).unwrap();

    // read the prefix meta from the offset, get the exact size to read from the offset, then read that whole thing and return
    let mut buffer = vec![0; PREFIX_META_SIZE as usize];
    fd.read_exact(&mut buffer).unwrap();

    let new_offset = offset + PREFIX_META_SIZE as u64;
     fd.seek(SeekFrom::Start(new_offset)).unwrap();
    let meta = deserialize_metadata(buffer);
    let actual_entry_size = meta.read_size;

    let mut ret_buffer = vec![0; actual_entry_size as usize];
    fd.read_exact(&mut ret_buffer).unwrap();

    ret_buffer
    // now, that buffer is just the raw bytes containing the serialized metadata for the entry
    // we need to deserialize it and read it, for now, let's just keep it human readable json
}

pub fn write_to_path(path: String, offset: u64, data: Vec<u8>) -> Result<(), io::Error> {
    let mut fd = File::create(path)?;
    fd.seek(SeekFrom::Start(offset))?;
    
    // Create metadata
    let data_size = data.len();
    let new_meta = Metadata { read_size: data_size };
    
    // Serialize and pad metadata to exactly PREFIX_META_SIZE
    let meta_json = serde_json::to_string(&new_meta).unwrap();
    let mut meta_buffer = vec![0u8; PREFIX_META_SIZE as usize];
    let meta_bytes = meta_json.as_bytes();
    meta_buffer[..meta_bytes.len()].copy_from_slice(meta_bytes);
    
    // Combine metadata + data in memory
    let mut combined = Vec::with_capacity(PREFIX_META_SIZE as usize + data.len());
    combined.extend_from_slice(&meta_buffer);
    combined.extend_from_slice(&data);
    
    // ONE write syscall for everything
    fd.write_all(&combined)?;
    
    Ok(())
}


================================================
FILE: src/page_updates.rs
================================================
/*
how the hell do we apply updates to a page

prereqs:
- it needs to be in cache
- the update patch needs to be available, how ??
    - what if the patch is simply bigger, 
        - like, can we somemhow manage a bit uneven size ? 



so what are the things like right now ?

first of all, cache heirarchies need to be present

like, if we have a page decompressed in PageCache , when it needs to be evicted, we would need to evict all its entries:
like: a Page is one thing, its entries is(are) multiple things

the moment a page is decompressed, we need to evict its compressed version from the `Compressed Page Cache`

Page Cache:
[Compressed Page Cache]
[Decompressed Page Cache]

Any cache in general should be:
- able to quickly check if a Page(whether compressed or uncompressed) is in memory or not

we can kinda use compressed Page references with the metadata store
*/

